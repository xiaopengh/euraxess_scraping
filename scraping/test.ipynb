{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c53956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as BS\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "import selenium.common.exceptions\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from time import sleep, time\n",
    "from tqdm import tqdm\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ecb6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36',\n",
    "        'Accept-Language': 'en-US,en;q=0.9,fr-FR;q=0.8,fr;q=0.7,zh-CN;q=0.6,zh;q=0.5',\n",
    "        'Referer': 'https://google.com',\n",
    "        'DNT': '1'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6422f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_oportunities(keywords):\n",
    "    # Header definition\n",
    "    # https://euraxess.ec.europa.eu/jobs/search?f%5B0%5D=keywords%3ALLM\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36',\n",
    "        'Accept-Language': 'en-US,en;q=0.9,fr-FR;q=0.8,fr;q=0.7,zh-CN;q=0.6,zh;q=0.5',\n",
    "        'Referer': 'https://google.com',\n",
    "        'DNT': '1'\n",
    "    }\n",
    "    # Main page\n",
    "    main_str = \"https://euraxess.ec.europa.eu\"\n",
    "    # Keywords (i.e. job field: data science, data analyst etc...)\n",
    "    keywords = str(keywords)\n",
    "    # Join keywords and main page: web to scrape\n",
    "    # Currently (11/2025), EURAXESS uses this format to search by keywords\n",
    "    search_str = '?f%5B0%5D=keywords%3A' + keywords.replace(\" \", \"%20\") + '\"'\n",
    "    subdomain_str = main_str + \"/jobs/search\" + search_str\n",
    "    # Subdomain request\n",
    "    subdomain_rq = requests.get(subdomain_str, headers=headers)\n",
    "    subdomain_rq.close()\n",
    "    # If web is available, it will return 200 (i.e. Ok for scraping)\n",
    "    if subdomain_rq.status_code == 200:\n",
    "        print(\"Web is available for scraping\")\n",
    "    else:\n",
    "        print(\"Something is wrong. Status code:\", subdomain_rq.status_code)\n",
    "    # Extract useful info from subdomain (keyword) page\n",
    "    return BS(subdomain_rq.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08af14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = \"LLM\"\n",
    "# Main page\n",
    "main_str = \"https://euraxess.ec.europa.eu\"\n",
    "# Keywords (i.e. job field: data science, data analyst etc...)\n",
    "# Join keywords and main page: web to scrape\n",
    "search_str = '?f%5B0%5D=keywords%3A' + keywords.replace(\" \", \"%20\") + '\"'\n",
    "subdomain_str = main_str + \"/jobs/search\" + search_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ad1000",
   "metadata": {},
   "outputs": [],
   "source": [
    "subdomain_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152125c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_subdomain = search_oportunities(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f032b2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of offers from the subdomain page\n",
    "offers_subdomain = soup_subdomain.find_all(\"h2\", id = \"search_results_count\")\n",
    "if offers_subdomain:\n",
    "    span = offers_subdomain[0].find(\"span\")\n",
    "else:\n",
    "    print(\"No offers found\")\n",
    "offers_count = re.findall(r'[0-9]+', str(span))[0] if span else \"0\"\n",
    "int(offers_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3678d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern to match number inside parentheses within span\n",
    "offers_count = re.findall(r'<span>\\s*\\((\\d+)\\)</span>', str(offers_subdomain))\n",
    "offers_count = int(offers_count[0])\n",
    "offers_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903a312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pager_subdomain = soup_subdomain.find_all(\"li\", {\"class\": \"ecl-pagination__item ecl-pagination__item--last\"})\n",
    "if pager_subdomain:\n",
    "    pages_count_subdomain = pager_subdomain[0].find(\"a\")\n",
    "else:\n",
    "    print(\"No pagination found\")\n",
    "pages_count_subdomain = re.findall(r'[0-9]+', str(pages_count_subdomain))[0]\n",
    "int(pages_count_subdomain)\n",
    "pages_count_subdomain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f924a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of offers using the keyword \"{}\":'.format(keywords), offers_count, \"in {} pages\".format(pages_count_subdomain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741b2d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "subdomain_pages_str = subdomain_str + \"&page={}\".format(1)  # String of keyword webpage i\n",
    "subdomain_pages_rq = requests.get(subdomain_pages_str, headers=headers, timeout=120)  # Request of this page\n",
    "subdomain_pages_rq.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b091217",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_subdomain_pages = BS(subdomain_pages_rq.content, \"html.parser\")  # page BeautifulSoup\n",
    "titles_subdomain = soup_subdomain_pages.find_all(\"h3\", {\"class\": \"ecl-content-block__title\"})  # Obtain html line of titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6e621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_subdomain_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e8dd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_href_sub = []\n",
    "raw_titles_sub = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f346e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for title in titles_subdomain:\n",
    "    suffix_href = \"\".join(re.findall(r'/jobs/\\d+',str(title)))\n",
    "    href = main_str + suffix_href\n",
    "    raw_href_sub.append(href)   # Get offer URLs\n",
    "    raw_titles_sub.append(title.get_text().replace(\"\\n\",\"\"))  # Get offer titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c686245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_href_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95005a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = raw_href_sub[0]\n",
    "job_rq = requests.get(url, headers=headers, timeout=120)\n",
    "job_rq.close()\n",
    "job_soup = BS(job_rq.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a112c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_job_data(job_soup):\n",
    "    \"\"\"\n",
    "    Extract structured job information from EURAXESS job page soup\n",
    "    \n",
    "    Parameters:\n",
    "    job_soup: BeautifulSoup object of the job page\n",
    "    \n",
    "    Returns:\n",
    "    dict: Structured job data\n",
    "    \"\"\"\n",
    "    \n",
    "    job_data = {}\n",
    "    \n",
    "    # Helper function to safely extract text from description lists\n",
    "    def get_dl_value(term_text):\n",
    "        \"\"\"Extract value from description list given the term\"\"\"\n",
    "        dl_items = job_soup.find_all('dt', class_='ecl-description-list__term')\n",
    "        for dt in dl_items:\n",
    "            if term_text.lower() in dt.get_text(strip=True).lower():\n",
    "                dd = dt.find_next_sibling('dd')\n",
    "                if dd:\n",
    "                    return dd.get_text(strip=True)\n",
    "        return None\n",
    "    \n",
    "    # Extract title\n",
    "    title_elem = job_soup.find('h1', class_='ecl-content-block__title')\n",
    "    job_data['title'] = title_elem.get_text(strip=True) if title_elem else None\n",
    "    \n",
    "    # Extract organization\n",
    "    org_link = job_soup.find('a', href=re.compile(r'/partnering/organisations/profile/'))\n",
    "    job_data['organization'] = org_link.get_text(strip=True) if org_link else None\n",
    "    \n",
    "    # Extract country from label\n",
    "    country_label = job_soup.find('span', class_='ecl-label--highlight')\n",
    "    job_data['country'] = country_label.get_text(strip=True) if country_label else None\n",
    "    \n",
    "    # Extract posted date\n",
    "    posted_meta = job_soup.find('li', string=re.compile(r'Posted on:'))\n",
    "    if posted_meta:\n",
    "        job_data['posted_date'] = posted_meta.get_text(strip=True).replace('Posted on:', '').strip()\n",
    "    else:\n",
    "        job_data['posted_date'] = None\n",
    "    \n",
    "    # Extract main job information from description lists\n",
    "    job_data['research_field'] = get_dl_value('Research Field')\n",
    "    job_data['researcher_profile'] = get_dl_value('Researcher Profile')\n",
    "    job_data['positions'] = get_dl_value('Positions')\n",
    "    job_data['application_deadline'] = get_dl_value('Application Deadline')\n",
    "    job_data['contract_type'] = get_dl_value('Type of Contract')\n",
    "    job_data['job_status'] = get_dl_value('Job Status')\n",
    "    job_data['hours_per_week'] = get_dl_value('Hours Per Week')\n",
    "    job_data['offer_starting_date'] = get_dl_value('Offer Starting Date')\n",
    "    job_data['eu_funded'] = get_dl_value('Is the job funded through the EU Research Framework Programme?')\n",
    "    job_data['reference_number'] = get_dl_value('Reference Number')\n",
    "    \n",
    "    # Extract offer description\n",
    "    offer_desc_section = job_soup.find('h2', id='offer-description')\n",
    "    if offer_desc_section:\n",
    "        desc_div = offer_desc_section.find_next('div', class_='ecl')\n",
    "        job_data['offer_description'] = desc_div.get_text(strip=True) if desc_div else None\n",
    "    else:\n",
    "        job_data['offer_description'] = None\n",
    "    \n",
    "    # Extract application URL\n",
    "    apply_link = job_soup.find('a', class_='job-apply-button')\n",
    "    job_data['application_url'] = apply_link['href'] if apply_link and 'href' in apply_link.attrs else None\n",
    "    \n",
    "    # Extract requirements\n",
    "    job_data['education_level'] = get_dl_value('Education Level')\n",
    "    job_data['languages'] = get_dl_value('Languages')\n",
    "    job_data['language_level'] = get_dl_value('Level')\n",
    "    job_data['years_experience'] = get_dl_value('Years of Research Experience')\n",
    "    \n",
    "    # Extract skills/qualifications\n",
    "    skills_section = job_soup.find('div', class_='ecl-u-type-bold', string=re.compile(r'Skills/Qualifications'))\n",
    "    if skills_section:\n",
    "        skills_div = skills_section.find_next('div', class_='ecl')\n",
    "        job_data['skills_qualifications'] = skills_div.get_text(strip=True) if skills_div else None\n",
    "    else:\n",
    "        job_data['skills_qualifications'] = None\n",
    "    \n",
    "    # Extract specific requirements\n",
    "    specific_req_section = job_soup.find('div', class_='ecl-u-type-bold', string=re.compile(r'Specific Requirements'))\n",
    "    if specific_req_section:\n",
    "        req_div = specific_req_section.find_next('div', class_='ecl')\n",
    "        job_data['specific_requirements'] = req_div.get_text(strip=True) if req_div else None\n",
    "    else:\n",
    "        job_data['specific_requirements'] = None\n",
    "    \n",
    "    # Extract benefits\n",
    "    benefits_section = job_soup.find('div', class_='ecl-u-type-bold', string=re.compile(r'Benefits'))\n",
    "    if benefits_section:\n",
    "        benefits_div = benefits_section.find_next('div', class_='ecl')\n",
    "        job_data['benefits'] = benefits_div.get_text(strip=True) if benefits_div else None\n",
    "    else:\n",
    "        job_data['benefits'] = None\n",
    "    \n",
    "    # Extract work location details\n",
    "    job_data['work_location_count'] = get_dl_value('Number of offers available')\n",
    "    job_data['work_company'] = get_dl_value('Company/Institute')\n",
    "    job_data['work_city'] = get_dl_value('City')\n",
    "    job_data['work_state'] = get_dl_value('State/Province')\n",
    "    job_data['work_postal_code'] = get_dl_value('Postal Code')\n",
    "    job_data['work_street'] = get_dl_value('Street')\n",
    "    \n",
    "    # Extract contact information\n",
    "    contact_email_dd = job_soup.find('dt', string=re.compile(r'E-Mail'))\n",
    "    if contact_email_dd:\n",
    "        email_dd = contact_email_dd.find_next_sibling('dd')\n",
    "        job_data['contact_email'] = email_dd.get_text(strip=True) if email_dd else None\n",
    "    else:\n",
    "        job_data['contact_email'] = None\n",
    "    \n",
    "    contact_website = job_soup.find('dt', string=re.compile(r'^Website$'))\n",
    "    if contact_website:\n",
    "        website_dd = contact_website.find_next_sibling('dd')\n",
    "        website_link = website_dd.find('a') if website_dd else None\n",
    "        job_data['contact_website'] = website_link['href'] if website_link and 'href' in website_link.attrs else None\n",
    "    else:\n",
    "        job_data['contact_website'] = None\n",
    "    \n",
    "    return job_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cb08a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_data = extract_job_data(job_soup)\n",
    "job_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c5b025",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1415a2",
   "metadata": {},
   "source": [
    "## Test $\\texttt{search\\_oportunities}$ function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8294a7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting search for opportunities with keywords: LLM\n",
      "Web is available for scraping\n",
      "Number of offers using the keyword \"LLM\": 43 in 5 pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:06<00:00,  1.28s/it]\n",
      "100%|██████████| 43/43 [01:56<00:00,  2.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed. Number of jobs scraped: 43\n"
     ]
    }
   ],
   "source": [
    "from utils import search_oportunities\n",
    "\n",
    "jobs_data = search_oportunities(keywords = \"LLM\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "euraxess_scraping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
